#!/bin/bash
#SBATCH --job-name=download
#SBATCH -p standard
#SBATCH -A vswarup_lab
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --error=slurm-%J.err
#SBATCH --mem 4G
#SBATCH --array=0-63
#SBATCH --time=72:00:00

# get a file with all of the paths (don't run this in the batch script, just run it before!!)
# wget --spider -r --no-parent http://hts.igb.uci.edu/emiyoshi21060178/ 2>&1 | grep .fastq | grep -v Removing | tr -s ' ' | cut -d ' ' -f 3 > bin/download_filepaths.txt

# output directory
output_dir="/dfs3b/swaruplab/smorabit/data/PiD_2021/June2021/"
cd $output_dir

# set index to the slurm task ID
let index="$SLURM_ARRAY_TASK_ID"

# get the current file to download from the filepaths txt file:
file=$(head -n $index /dfs3b/swaruplab/smorabit/data/PiD_2021/June2021/bin/download_filepaths.txt | tail -n 1)
echo $file

# download the file!!!
wget $file
