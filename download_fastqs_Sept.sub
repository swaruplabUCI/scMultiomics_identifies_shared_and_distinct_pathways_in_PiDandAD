#!/bin/bash
#SBATCH --job-name=download
#SBATCH -p standard
#SBATCH -A vswarup_lab
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --error=slurm-%J.err
#SBATCH --mem 4G
#SBATCH --array=0-16
#SBATCH --time=4:00:00

# get a file with all of the paths (don't run this in the batch script, just run it before!!)
# wget --spider -r --no-parent http://hts.igb.uci.edu/sudeshnd21092055/ 2>&1 | grep .txt.gz | grep -v Removing | tr -s ' ' | cut -d ' ' -f 3 > bin/download_filepaths.txt

# output directory
output_dir="/dfs3b/swaruplab/smorabit/data/PiD_2021/Sept2021/"
cd $output_dir

# set index to the slurm task ID
let index="$SLURM_ARRAY_TASK_ID"

# get the current file to download from the filepaths txt file:
file=$(head -n $index /dfs3b/swaruplab/smorabit/data/PiD_2021/Sept2021/bin/download_filepaths.txt | tail -n 1)
echo $file

# download the file!!!
wget $file


# md5 checksums for output files:
md5_file="bin/downloaded_md5.txt"
touch $md5_file
for file in fastqs/*; do
  echo $file
  md5sum $file >> $md5_file
done

# comparing
cat bin/md5sum_lane4-group6.txt bin/downloaded_md5.txt | cut -f 1 -d ' ' | sort | uniq -c
